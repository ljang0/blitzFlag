---
title: "modeling"
author: "Lawrence Jang"
date: "1/28/2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Read in the data

read in our csvs from the Big Data Bowl 2023 dataset

```{r}
games = read.csv("games.csv")
plays = read.csv("plays.csv")
players = read.csv("players.csv")
pff = read.csv("pffScoutingData.csv")
week1 = read.csv("week1.csv")
week2 = read.csv("week2.csv")
week3 = read.csv("week3.csv")
week4 = read.csv("week4.csv")
week5 = read.csv("week5.csv")
week6 = read.csv("week6.csv")
week7 = read.csv("week7.csv")
week8 = read.csv("week8.csv")
#merge all the weeks together
allWeeks = rbind(week1, week2, week3, week4, week5, week6, week7, week8)
allWeeks = allWeeks %>% mutate(
    x = ifelse(playDirection == "left", 120 - x, x),
    y = ifelse(playDirection == "left", 160 / 3 - y, y),
    # flip player direction and orientation
    dir = ifelse(playDirection == "left", dir + 180, dir),
    dir = ifelse(dir > 360, dir - 360, dir),
    o = ifelse(playDirection == "left", o + 180, o),
    o = ifelse(o > 360, o - 360, o)
  )
```


# MODELING:

# Making a Baseline

# ALL WEEK DATASET

```{r}
library(dplyr)
allWeeks = allWeeks %>% mutate(
    x = ifelse(playDirection == "left", 120 - x, x),
    y = ifelse(playDirection == "left", 160 / 3 - y, y),
    # flip player direction and orientation
    dir = ifelse(playDirection == "left", dir + 180, dir),
    dir = ifelse(dir > 360, dir - 360, dir),
    o = ifelse(playDirection == "left", o + 180, o),
    o = ifelse(o > 360, o - 360, o)
  )
# merge all week 1 data with player background data
weeksPlayerInfo = merge(allWeeks, players, by = c("nflId"), all.x = TRUE)
# merge all week 1 frames with PFF data for each play
weeksPlayerInfo = merge(weeksPlayerInfo, pff, by = c("gameId", "playId", "nflId"), all.x = TRUE)
# merge with play level data
weeksPlayerInfo = merge(weeksPlayerInfo, plays, by = c("gameId", "playId"), all.x = TRUE)
#w1ballsnaps = week1 %>% filter(event == "ball_snap")
# filter for all centers
weeksSnaps = weeksPlayerInfo %>% filter(event == "ball_snap" & pff_positionLinedUp == "C")
weeksSnaps$centerX = weeksSnaps$x
weeksSnaps$centerY = weeksSnaps$y
weeksSnaps = weeksSnaps %>% select(c("playId", "gameId", "centerX", "centerY"))
# get the first frozen shot
zeroFrameAll = weeksPlayerInfo %>% filter(frameId == 1)
# add center x,y
zeroFrameAll = merge(zeroFrameAll, weeksSnaps, by = c("gameId", "playId"), all.x = TRUE)
# add team and play level data
#zeroFrameFull = merge(zeroFrame, mergedPlays, by = c("gameId","playId"), all.x = TRUE)
# filter for defensive players
defenseZeroFrameAll <- zeroFrameAll %>%filter(team != possessionTeam)
# code the outcome variable and factor variables
#head(defenseZeroFrame)
defenseZeroFrameAll$label <- ifelse(defenseZeroFrameAll$pff_role == "Pass Rush", 1, 0)
defenseZeroFrameAll$label <- factor(defenseZeroFrameAll$label)
#colnames(defenseZeroFrame2)
# get distance from snap of ball
defenseZeroFrameAll$xDiff = abs(defenseZeroFrameAll$centerX - defenseZeroFrameAll$x)
defenseZeroFrameAll$yDiff = abs(defenseZeroFrameAll$centerY - defenseZeroFrameAll$y)
defenseZeroFrameAll = defenseZeroFrameAll[complete.cases(defenseZeroFrameAll$label), ]
```

### Majority Class Prediction

```{r}
majority_labels <- defenseZeroFrameAll %>%
  group_by(pff_positionLinedUp) %>%
  summarise(MajorityLabel = names(which.max(table(label))),
            .groups = 'drop')
defenseZeroFrameAll <- defenseZeroFrameAll %>%
  left_join(majority_labels, by = "pff_positionLinedUp") %>%
  mutate(MajorityClassPrediction = as.numeric(MajorityLabel)) %>%
  select(-MajorityLabel) # Remove the MajorityLabel column after adding Prediction

# Assuming the actual label column in zeroDefenseFrameAll is named `Label`

# Step 2: Calculate accuracy
accuracy <- sum(defenseZeroFrameAll$label == defenseZeroFrameAll$MajorityClassPrediction) / nrow(defenseZeroFrameAll)
print(paste("Accuracy:", accuracy))


# get stats grouped
defenseZeroFrameAll <- defenseZeroFrameAll %>%
  mutate(CorrectMajorityPrediction = label == MajorityClassPrediction)
metrics_by_group <- defenseZeroFrameAll %>%
  group_by(pff_positionLinedUp) %>%
  summarise(
    Accuracy = mean(CorrectMajorityPrediction),
    Precision = sum(label == 1 & MajorityClassPrediction == 1) / sum(MajorityClassPrediction == 1),
    Recall = sum(label == 1 & MajorityClassPrediction == 1) / sum(label == 1),
    truePosCount = sum(MajorityClassPrediction == 1),
    posCount = sum(label == 1),
    .groups = 'drop' # Drop grouping for the output
  )
print(metrics_by_group)

```

### Decision Tree


### Decision Tree using simply position

```{r}
# decision tree
library(rpart)
library(caret)
library(lme4)
unique_gameids <- unique(defenseZeroFrameAll$gameId)
# Set the proportion for the train set
train_proportion <- 0.7
# Calculate the number of gameids for the train set
num_train_gameids <- ceiling(length(unique_gameids) * train_proportion)
# Randomly select gameids for the train set
train_gameids <- sample(unique_gameids, num_train_gameids)
# Split the data based on the selected gameids
train_data <- defenseZeroFrameAll[defenseZeroFrameAll$gameId %in% train_gameids, ]
test_data <- defenseZeroFrameAll[!(defenseZeroFrameAll$gameId %in% train_gameids), ]
# Print the summary of the model
# Convert probabilities to predicted classes (0 or 1)
tree_model <- rpart(label ~ pff_positionLinedUp, data = train_data, method = "class")
predictions <- predict(tree_model, test_data, type = "class")
test_data$predictions = predictions
conf_matrix <- confusionMatrix(predictions, test_data$label)
# Display the confusion matrix
print(conf_matrix)

# Extract accuracy, precision, recall, and F1 score
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- conf_matrix$byClass["F1"]

# Print the results
cat("Accuracy:", round(accuracy, 2), "\n")
cat("Precision:", round(precision, 2), "\n")
cat("Recall (Sensitivity):", round(recall, 2), "\n")
cat("F1 Score:", round(f1_score, 2), "\n")
```

let's try grouping by position.

```{r}
# get stats grouped
test_data <- test_data %>%
  mutate(correctPrediction = label == predictions)
metrics_by_group <- test_data %>%
  group_by(pff_positionLinedUp) %>%
  summarise(
    Accuracy = mean(correctPrediction),
    Precision = sum(label == 1 & predictions == 1) / sum(predictions == 1),
    Recall = sum(label == 1 & predictions == 1) / sum(label == 1),
    truePosCount = sum(predictions == 1),
    posCount = sum(label == 1),
    .groups = 'drop' # Drop grouping for the output
  )
print(metrics_by_group)
```


decision tree with x, y tracking data, defenders in box, yards to go, down, position
```{r}
# decision tree
library(rpart)
library(caret)
library(lme4)
unique_gameids <- unique(defenseZeroFrameAll$gameId)
# Set the proportion for the train set
train_proportion <- 0.7
# Calculate the number of gameids for the train set
num_train_gameids <- ceiling(length(unique_gameids) * train_proportion)
# Randomly select gameids for the train set
#train_gameids <- sample(unique_gameids, num_train_gameids)
# Split the data based on the selected gameids
train_data <- defenseZeroFrameAll[defenseZeroFrameAll$gameId %in% train_gameids, ]
test_data <- defenseZeroFrameAll[!(defenseZeroFrameAll$gameId %in% train_gameids), ]
# Print the summary of the model
# Convert probabilities to predicted classes (0 or 1)
tree_model <- rpart(label ~ xDiff + yDiff + yardsToGo + pff_positionLinedUp + defendersInBox + down, data = train_data, method = "class")
#tree_model <- rpart(label ~ xDiff + yDiff + yardsToGo + defendersInBox + down, data = train_data, method = "class")
#tree_model <- rpart(label ~ nflId + personnelD + team,  data = train_data, method = "class")
predictions <- predict(tree_model, test_data, type = "class")
test_data$predictions = predictions
conf_matrix <- confusionMatrix(predictions, test_data$label)
# Display the confusion matrix
print(conf_matrix)

# Extract accuracy, precision, recall, and F1 score
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- conf_matrix$byClass["F1"]

# Print the results
cat("Accuracy:", round(accuracy, 2), "\n")
cat("Precision:", round(precision, 2), "\n")
cat("Recall (Sensitivity):", round(recall, 2), "\n")
cat("F1 Score:", round(f1_score, 2), "\n")
```
```{r}
# get stats grouped
test_data <- test_data %>%
  mutate(correctPrediction = label == predictions)
metrics_by_group <- test_data %>%
  group_by(pff_positionLinedUp) %>%
  summarise(
    Accuracy = mean(correctPrediction),
    Precision = sum(label == 1 & predictions == 1) / sum(predictions == 1),
    Recall = sum(label == 1 & predictions == 1) / sum(label == 1),
    truePosCount = sum(predictions == 1),
    posCount = sum(label == 1),
    .groups = 'drop' # Drop grouping for the output
  )
print(metrics_by_group)
```

let's try adding direction and orientation:

```{r}
# decision tree
library(rpart)
library(caret)
library(lme4)
unique_gameids <- unique(defenseZeroFrameAll$gameId)
# Set the proportion for the train set
train_proportion <- 0.7
# Calculate the number of gameids for the train set
num_train_gameids <- ceiling(length(unique_gameids) * train_proportion)
# Randomly select gameids for the train set
#train_gameids <- sample(unique_gameids, num_train_gameids)
# Split the data based on the selected gameids
train_data <- defenseZeroFrameAll[defenseZeroFrameAll$gameId %in% train_gameids, ]
test_data <- defenseZeroFrameAll[!(defenseZeroFrameAll$gameId %in% train_gameids), ]
tree_model <- rpart(label ~ xDiff + yDiff + yardsToGo + pff_positionLinedUp + defendersInBox + down + dir + o, data = train_data, method = "class")

predictions <- predict(tree_model, test_data, type = "class")
test_data$predictions = predictions
conf_matrix <- confusionMatrix(predictions, test_data$label)
# Display the confusion matrix
print(conf_matrix)

# Extract accuracy, precision, recall, and F1 score
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- conf_matrix$byClass["F1"]

# Print the results
cat("Accuracy:", round(accuracy, 2), "\n")
cat("Precision:", round(precision, 2), "\n")
cat("Recall (Sensitivity):", round(recall, 2), "\n")
cat("F1 Score:", round(f1_score, 2), "\n")
```
```{r}
# get stats grouped
test_data <- test_data %>%
  mutate(correctPrediction = label == predictions)
metrics_by_group <- test_data %>%
  group_by(pff_positionLinedUp) %>%
  summarise(
    Accuracy = mean(correctPrediction),
    Precision = sum(label == 1 & predictions == 1) / sum(predictions == 1),
    Recall = sum(label == 1 & predictions == 1) / sum(label == 1),
    truePosCount = sum(predictions == 1),
    posCount = sum(label == 1),
    .groups = 'drop' # Drop grouping for the output
  )
print(metrics_by_group)
```

## More Modeling

### XGBoost Models

base level xgboost model with the following variables:
  xDiff
  yDiff
  yardsToGo
  personnelD
  personnelO
  defendersInBox
  dir
  o

```{r}
library(xgboost)
library(caret)

unique_gameids <- unique(defenseZeroFrameAll$gameId)
defenseZeroFrameAll$personnelD = as.integer(as.factor(defenseZeroFrameAll$personnelD))
defenseZeroFrameAll$personnelO = as.integer(as.factor(defenseZeroFrameAll$personnelO))
defenseZeroFrameAll$pff_positionLinedUp = as.integer(as.factor(defenseZeroFrameAll$pff_positionLinedUp))
# Set the proportion for the train set
train_proportion <- 0.7
# Calculate the number of gameids for the train set
num_train_gameids <- ceiling(length(unique_gameids) * train_proportion)
# Randomly select gameids for the train set
#train_gameids <- sample(unique_gameids, num_train_gameids)
# Split the data based on the selected gameids
train_data <- defenseZeroFrameAll[defenseZeroFrameAll$gameId %in% train_gameids, ]
test_data <- defenseZeroFrameAll[!(defenseZeroFrameAll$gameId %in% train_gameids), ]
train_data <- train_data[complete.cases(train_data$label), ]
test_data <- test_data[complete.cases(test_data$label), ]
train_data$label = as.numeric(as.character(train_data$label))
test_data$label = as.numeric(as.character(test_data$label))


dtrain <- xgb.DMatrix(data = as.matrix(train_data[, c("xDiff", "yDiff", "yardsToGo", "pff_positionLinedUp", "personnelD", "personnelO", "defendersInBox", 'dir', 'o')]), label = train_data$label)
dtest <- xgb.DMatrix(data = as.matrix(test_data[,c("xDiff", "yDiff", "yardsToGo","pff_positionLinedUp", "personnelD", "personnelO", "defendersInBox", 'dir', 'o')]), label = test_data$label)

params <- list(
  objective = "binary:logistic",  # Binary classification problem
  eval_metric = "logloss",        # Logarithmic loss as evaluation metric
  max_depth = 6,                  # Maximum depth of a tree
  eta = 0.3,                      # Learning rate
  subsample = 0.8,                # Fraction of observations to be randomly sampled for each tree
  colsample_bytree = 0.8          # Fraction of features to be randomly sampled for each tree
)

# Train the XGBoost model
xgb_model <- xgboost(
  params = params,
  data = dtrain,
  nrounds = 100,                   # Number of boosting rounds
  early_stopping_rounds = 10,      # Stop if performance does not improve for this many rounds
  verbose = 1                      # Print messages during training
)

# Make predictions on the test set
predictions <- predict(xgb_model, dtest)

# Convert predicted probabilities to binary predictions (0 or 1)
binary_predictions <- ifelse(predictions > 0.5, 1, 0)

# Evaluate the model performance (you can use other metrics as well)
conf_matrix <- table(binary_predictions, test_data$label)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
conf_matrix
cat("Accuracy:", accuracy, "\n")
```

### Multilevel Models

Random Intercept Model:
```{r}
library(caret)
library(lme4)
unique_gameids <- unique(defenseZeroFrameAll$gameId)
# Set the proportion for the train set
train_proportion <- 0.7
# Calculate the number of gameids for the train set
num_train_gameids <- ceiling(length(unique_gameids) * train_proportion)
# Randomly select gameids for the train set
train_gameids <- sample(unique_gameids, num_train_gameids)
# Split the data based on the selected gameids
train_data <- defenseZeroFrameAll[defenseZeroFrameAll$gameId %in% train_gameids, ]
test_data <- defenseZeroFrameAll[!(defenseZeroFrameAll$gameId %in% train_gameids), ]
train_data <- train_data[complete.cases(train_data$label), ]
test_data <- test_data[complete.cases(test_data$label), ]
mlmIntercepts <- glmer(label ~ 1 + (1 | nflId) + (1 | personnelD) + (1 | team), data = train_data, family = binomial)

### FOR RANDOM INTERCEPTS:

# Print the summary of the model
summary(mlmIntercepts)

test_data$predicted_probabilities <- predict(mlmIntercepts, newdata = test_data, type = "response", allow.new.levels = TRUE)

# Convert probabilities to predicted classes (0 or 1)
test_data$predicted_classes <- as.factor(ifelse(test_data$predicted_probabilities >= 0.5, 1, 0))

# Calculate accuracy
accuracy <- mean(test_data$label == test_data$predicted_classes, na.rm = TRUE)

conf_matrix <- confusionMatrix(test_data$predicted_classes, test_data$label)
# Display the confusion matrix
print(conf_matrix)

# Extract accuracy, precision, recall, and F1 score
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- conf_matrix$byClass["F1"]

# Print the results
cat("Accuracy:", round(accuracy, 2), "\n")
cat("Precision:", round(precision, 2), "\n")
cat("Recall (Sensitivity):", round(recall, 2), "\n")
cat("F1 Score:", round(f1_score, 2), "\n")
```

Random Intercept Models + Coords

```{r}
mlmCoords  <- glmer(label ~ 1 + xDiff + yDiff + (1 | nflId) + (1 | personnelD) + (1 | team), data = train_data, family = binomial)
### FOR RANDOM INTERCEPTS:

# Print the summary of the model
summary(mlmCoords)

test_data$predicted_probabilities <- predict(mlmCoords, newdata = test_data, type = "response", allow.new.levels = TRUE)

# Convert probabilities to predicted classes (0 or 1)
test_data$predicted_classes <- as.factor(ifelse(test_data$predicted_probabilities >= 0.5, 1, 0))

# Calculate accuracy
accuracy <- mean(test_data$label == test_data$predicted_classes, na.rm = TRUE)

conf_matrix <- confusionMatrix(test_data$predicted_classes, test_data$label)
# Display the confusion matrix
print(conf_matrix)

# Extract accuracy, precision, recall, and F1 score
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- conf_matrix$byClass["F1"]

# Print the results
cat("Accuracy:", round(accuracy, 2), "\n")
cat("Precision:", round(precision, 2), "\n")
cat("Recall (Sensitivity):", round(recall, 2), "\n")
cat("F1 Score:", round(f1_score, 2), "\n")
```

Random Intercept Models + Coords + Orientation + Direction
```{r}
mlmCoordsFull <- glmer(label ~ 1 + xDiff + yDiff + dir + o + (1 | nflId) + (1 | personnelD) + (1 | team), data = train_data, family = binomial)
### FOR RANDOM INTERCEPTS:

# Print the summary of the model
summary(mlmCoordsFull)

test_data$predicted_probabilities <- predict(mlmCoordsFull, newdata = test_data, type = "response", allow.new.levels = TRUE)

# Convert probabilities to predicted classes (0 or 1)
test_data$predicted_classes <- as.factor(ifelse(test_data$predicted_probabilities >= 0.5, 1, 0))

# Calculate accuracy
accuracy <- mean(test_data$label == test_data$predicted_classes, na.rm = TRUE)

conf_matrix <- confusionMatrix(test_data$predicted_classes, test_data$label)
# Display the confusion matrix
print(conf_matrix)

# Extract accuracy, precision, recall, and F1 score
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- conf_matrix$byClass["F1"]

# Print the results
cat("Accuracy:", round(accuracy, 2), "\n")
cat("Precision:", round(precision, 2), "\n")
cat("Recall (Sensitivity):", round(recall, 2), "\n")
cat("F1 Score:", round(f1_score, 2), "\n")
```

### Extra feature manipulation

Let's calculate the average x,y coords for lbs and dline for a given play.

```{r}
linebackers = c("LILB","ROLB", "RLB","RILB","LLB","LOLB", "MLB")
dline = c("DRT","RE", "LEO", "DLT","REO", "NLT" ,  "LE",  "NT",  "NRT")

averagePositionsLB <- defenseZeroFrameAll %>%
  filter(pff_positionLinedUp %in% linebackers) %>%
  group_by(gameId, playId) %>%
  summarise(lb_average_x = mean(x, na.rm = TRUE),
            lb_average_y = mean(y, na.rm = TRUE),
            .groups = 'drop') # Ensures the result is not grouped

# Step 2: Join this back to the original dataframe to add the average_x and average_y columns
# Note: Every row corresponding to the same gameId and playId will have the same average_x and average_y values
# If a playId does not include any linebackers, it will have NA for these averages 
defenseZeroFrameAll <- defenseZeroFrameAll %>%
  left_join(averagePositionsLB, by = c("gameId", "playId"))

averagePositionsDL <- defenseZeroFrameAll %>%
  filter(pff_positionLinedUp %in% dline) %>%
  group_by(gameId, playId) %>%
  summarise(dl_average_x = mean(x, na.rm = TRUE),
            dl_average_y = mean(y, na.rm = TRUE),
            .groups = 'drop') # Ensures the result is not grouped
defenseZeroFrameAll <- defenseZeroFrameAll %>%
  left_join(averagePositionsDL, by = c("gameId", "playId"))


defenseZeroFrameAll$lineDiffX = abs(defenseZeroFrameAll$dl_average_x - defenseZeroFrameAll$x)
defenseZeroFrameAll$lineDiffY = abs(defenseZeroFrameAll$dl_average_y - defenseZeroFrameAll$y)
defenseZeroFrameAll$lbDiffX = abs(defenseZeroFrameAll$lb_average_x - defenseZeroFrameAll$x)
defenseZeroFrameAll$lbDiffY = abs(defenseZeroFrameAll$lb_average_y - defenseZeroFrameAll$y)
```


## Linebackers focus

We need to shift to linebackers...the problem is too complex for a single
model to capture what each type of position does to signify a blitz.

```{r}
linebackersDF <- defenseZeroFrameAll[defenseZeroFrameAll$pff_positionLinedUp %in% linebackers, ]
```

Let's run our models using our new focus group of linebackers.

```{r}
unique_gameids <- unique(linebackersDF$gameId)
originalLevelsPersonnelD = levels(as.factor(linebackersDF$personnelD))
originalLevelsPersonnelO = levels(as.factor(linebackersDF$personnelO))
linebackersDF$personnelD = as.integer(as.factor(linebackersDF$personnelD))
linebackersDF$personnelO = as.integer(as.factor(linebackersDF$personnelO))
originalLevelsPositions = levels(as.factor(linebackersDF$pff_positionLinedUp))
linebackersDF$pff_positionLinedUp = as.integer(as.factor(linebackersDF$pff_positionLinedUp))
linebackersDF$pff_positionLinedUp_str <- originalLevelsPositions[linebackersDF$pff_positionLinedUp]
```


```{r}
# Set the proportion for the train set
train_proportion <- 0.7
# Calculate the number of gameids for the train set
num_train_gameids <- ceiling(length(unique_gameids) * train_proportion)

#train_data <- linebackersDF[linebackersDF$gameId %in% train_gameids, ]
#test_data <- linebackersDF[!(linebackersDF$gameId %in% train_gameids), ]
#train_data <- train_data[complete.cases(train_data$label), ]
#test_data <- test_data[complete.cases(test_data$label), ]
train_data$label = as.numeric(as.character(train_data$label))
test_data$label = as.numeric(as.character(test_data$label))

#predictors = c("xDiff", "yDiff", "lineDiffX", "lineDiffY", "lbDiffX", "lbDiffY", "yardsToGo", "pff_positionLinedUp", "personnelD", "personnelO", "defendersInBox", 'dir', 'o')
predictors = c("xDiff", "yDiff", "pff_positionLinedUp", "personnelD", "personnelO", "defendersInBox", 'dir', 'o')
dtrain <- xgb.DMatrix(data = as.matrix(train_data[, predictors]), label = train_data$label)
dtest <- xgb.DMatrix(data = as.matrix(test_data[,predictors]), label = test_data$label)

params <- list(
  objective = "binary:logistic",  # Binary classification problem
  eval_metric = "logloss",        # Logarithmic loss as evaluation metric
  max_depth = 6,                  # Maximum depth of a tree
  eta = 0.3,                      # Learning rate
  subsample = 0.8,                # Fraction of observations to be randomly sampled for each tree
  colsample_bytree = 0.8          # Fraction of features to be randomly sampled for each tree
)

# Train the XGBoost model
xgb_model <- xgboost(
  params = params,
  data = dtrain,
  nrounds = 100,                   # Number of boosting rounds
  early_stopping_rounds = 10,      # Stop if performance does not improve for this many rounds
  verbose = 1                      # Print messages during training
)

# Make predictions on the test set
predictions <- predict(xgb_model, dtest)

# Convert predicted probabilities to binary predictions (0 or 1)
binary_predictions <- ifelse(predictions > 0.5, 1, 0)
test_data$predictions = binary_predictions
# Evaluate the model performance (you can use other metrics as well)
conf_matrix <- confusionMatrix(as.factor(binary_predictions), as.factor(test_data$label))
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- conf_matrix$byClass["F1"]

# Print the results
cat("Accuracy:", round(accuracy, 2), "\n")
cat("Precision:", round(precision, 2), "\n")
cat("Recall (Sensitivity):", round(recall, 2), "\n")
cat("F1 Score:", round(f1_score, 2), "\n")
```

```{r}
# get stats grouped
test_data <- test_data %>%
  mutate(correctPrediction = label == predictions)
metrics_by_group <- test_data %>%
  group_by(pff_positionLinedUp_str ) %>%
  summarise(
    Accuracy = mean(correctPrediction),
    Precision = sum(label == 1 & predictions == 1) / sum(predictions == 1),
    Recall = sum(label == 1 & predictions == 1) / sum(label == 1),
    truePosCount = sum(predictions == 1),
    posCount = sum(label == 1),
    total = n(),
    .groups = 'drop' # Drop grouping for the output
  )
print(metrics_by_group)
```

let's get an honest baseline just using positions within linebacker groups.


```{r}
# decision tree
library(rpart)
library(caret)
library(lme4)
# Print the summary of the model
# Convert probabilities to predicted classes (0 or 1)
tree_model <- rpart(label ~ xDiff + yDiff, data = train_data, method = "class")
predictions <- predict(tree_model, test_data, type = "class")
test_data$predictionsNaive = predictions
conf_matrix <- confusionMatrix(as.factor(test_data$predictionsNaive), as.factor(test_data$label))
# Display the confusion matrix
print(conf_matrix)

# Extract accuracy, precision, recall, and F1 score
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- conf_matrix$byClass["F1"]

# Print the results
cat("Accuracy:", round(accuracy, 2), "\n")
cat("Precision:", round(precision, 2), "\n")
cat("Recall (Sensitivity):", round(recall, 2), "\n")
cat("F1 Score:", round(f1_score, 2), "\n")
```


```{r}
# get stats grouped
test_data <- test_data %>%
  mutate(correctPredictionNaive = label == predictionsNaive)
metrics_by_group <- test_data %>%
  group_by(pff_positionLinedUp_str ) %>%
  summarise(
    Accuracy = mean(correctPredictionNaive),
    Precision = sum(label == 1 & predictionsNaive == 1) / sum(predictionsNaive == 1),
    Recall = sum(label == 1 & predictionsNaive == 1) / sum(label == 1),
    truePosCount = sum(predictionsNaive == 1),
    posCount = sum(label == 1),
    total = n(),
    .groups = 'drop' # Drop grouping for the output
  )
print(metrics_by_group)
```


```{r}
mlmCoordsLB  <- glmer(label ~ 1 + xDiff + yDiff + (1 | nflId) + (1 | personnelD) + (1 | team), data = train_data, family = binomial)
### FOR RANDOM INTERCEPTS:

# Print the summary of the model
summary(mlmCoordsLB)

test_data$predicted_probabilities <- predict(mlmCoords, newdata = test_data, type = "response", allow.new.levels = TRUE)

# Convert probabilities to predicted classes (0 or 1)

test_data$predicted_classes <- as.factor(ifelse(test_data$predicted_probabilities >= 0.5, 1, 0))

# Calculate accuracy
accuracy <- mean(test_data$label == test_data$predicted_classes, na.rm = TRUE)

conf_matrix <- confusionMatrix(as.factor(test_data$predicted_classes), as.factor(test_data$label))
# Display the confusion matrix
print(conf_matrix)

# Extract accuracy, precision, recall, and F1 score
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- conf_matrix$byClass["F1"]

# Print the results
cat("Accuracy:", round(accuracy, 2), "\n")
cat("Precision:", round(precision, 2), "\n")
cat("Recall (Sensitivity):", round(recall, 2), "\n")
cat("F1 Score:", round(f1_score, 2), "\n")
```


```{r}
mlmBaseLB  <- glmer(label ~ 1 + (1 | nflId) + (1 | personnelD) + (1 | team), data = train_data, family = binomial)
### FOR RANDOM INTERCEPTS:

# Print the summary of the model
summary(mlmBaseLB)

test_data$predicted_probabilities <- predict(mlmBaseLB, newdata = test_data, type = "response", allow.new.levels = TRUE)

# Convert probabilities to predicted classes (0 or 1)

test_data$predicted_classes <- as.factor(ifelse(test_data$predicted_probabilities >= 0.5, 1, 0))

# Calculate accuracy
accuracy <- mean(test_data$label == test_data$predicted_classes, na.rm = TRUE)

conf_matrix <- confusionMatrix(as.factor(test_data$predicted_classes), as.factor(test_data$label))
# Display the confusion matrix
print(conf_matrix)

# Extract accuracy, precision, recall, and F1 score
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- conf_matrix$byClass["F1"]

# Print the results
cat("Accuracy:", round(accuracy, 2), "\n")
cat("Precision:", round(precision, 2), "\n")
cat("Recall (Sensitivity):", round(recall, 2), "\n")
cat("F1 Score:", round(f1_score, 2), "\n")
```

```{r}
# get stats grouped
test_data <- test_data %>%
  mutate(correctPredictionMLM = label == predicted_classes)
metrics_by_group <- test_data %>%
  group_by(pff_positionLinedUp_str ) %>%
  summarise(
    Accuracy = mean(correctPredictionMLM, na.rm = TRUE),
    Precision = sum(label == 1 & predicted_classes == 1, na.rm = TRUE) / sum(predicted_classes == 1, na.rm = TRUE),
    Recall = sum(label == 1 & predicted_classes == 1, na.rm = TRUE) / sum(label == 1, na.rm = TRUE),
    truePosCount = sum(predicted_classes == 1,na.rm = TRUE),
    posCount = sum(label == 1, na.rm = TRUE),
    total = n(),
    .groups = 'drop' # Drop grouping for the output
  )
print(metrics_by_group)
```

